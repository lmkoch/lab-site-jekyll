@article{djoumessi2023sparse,
   author = {Kerol R Donteu Djoumessi and Indu Ilanchezian and Laura Kühlewein and Hanna Faber and Christian F Baumgartner and Bubacarr Bah and Philipp Berens and Lisa M Koch},
   journal = {Proceedings of Machine Learning Research},
   keywords = {diabetic retinopathy,interpretability,sparse activations},
   pages = {1-17},
   title = {Sparse Activations for Interpretable Disease Grading},
   volume = {6},
   year = {2023},
}

@inproceedings{sun2023right,
   abstract = {While deep neural network models offer unmatched classification performance, they are prone to learning spurious correlations in the data. Such dependencies on confounding information can be difficult to detect using performance metrics if the test data comes from the same distribution as the training data. Interpretable ML methods such as post-hoc explanations or inherently interpretable classifiers promise to identify faulty model reasoning. However, there is mixed evidence whether many of these techniques are actually able to do so. In this paper, we propose a rigorous evaluation strategy to assess an explanation technique’s ability to correctly identify spurious correlations. Using this strategy, we evaluate five post-hoc explanation techniques and one inherently interpretable method for their ability to detect three types of artificially added confounders in a chest x-ray diagnosis task. We find that the post-hoc technique SHAP, as well as the inherently interpretable Attri-Net provide the best performance and can be used to reliably identify faulty model behavior.},
   author = {Susu Sun and Lisa M. Koch and Christian F. Baumgartner},
   doi = {10.1007/978-3-031-43895-0},
   isbn = {9783031438943},
   issn = {16113349},
   journal = {Proc. International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)},
   keywords = {Confounder detection,Interpretable machine learning},
   pages = {425-434},
   title = {Right for the Wrong Reason: Can Interpretable ML Techniques Detect Spurious Correlations?},
   volume = {14221 LNCS},
   year = {2023},
}
@inproceedings{sun2023inherently,
   abstract = {Interpretability is essential for machine learning algorithms in high-stakes application fields such as medical image analysis. However, high-performing black-box neural networks do not provide explanations for their predictions, which can lead to mistrust and suboptimal human-ML collaboration. Post-hoc explanation techniques, which are widely used in practice , have been shown to suffer from severe conceptual problems. Furthermore, as we show in this paper, current explanation techniques do not perform adequately in the multi-label scenario, in which multiple medical findings may co-occur in a single image. We propose Attri-Net 1 , an inherently interpretable model for multi-label classification. Attri-Net is a powerful classifier that provides transparent, trustworthy, and human-understandable explanations. The model first generates class-specific attribution maps based on counter-factuals to identify which image regions correspond to certain medical findings. Then a simple logistic regression classifier is used to make predictions based solely on these at-tribution maps. We compare Attri-Net to five post-hoc explanation techniques and one inherently interpretable classifier on three chest X-ray datasets. We find that Attri-Net produces high-quality multi-label explanations consistent with clinical knowledge and has comparable classification performance to state-of-the-art classification models.},
   author = {Susu Sun and Stefano Woerner and Andreas Maier and Lisa M. Koch and Christian F. Baumgartner},
   issn = {2640-3498},
   journal = {Proceedings of Machine Learning Research},
   keywords = {Interpretable Machine Learning,Multi-label Clas-sification,Visual Feature Attribution},
   month = {1},
   publisher = {PMLR},
   title = {Inherently Interpretable Multi-Label Classification Using Class-Specific Counterfactuals},
   volume = {227},
   year = {2023},
}


@inproceedings{boreiko2022visual,
   abstract = {In medical image classification tasks like the detection of diabetic retinopathy from retinal fundus images, it is highly desirable to get visual explanations for the decisions of black-box deep neural networks (DNNs). However, gradient-based saliency methods often fail to highlight the diseased image regions reliably. On the other hand, adversarially robust models have more interpretable gradients than plain models but suffer typically from a significant drop in accuracy, which is unacceptable for clinical practice. Here, we show that one can get the best of both worlds by ensembling a plain and an adversarially robust model: maintaining high accuracy but having improved visual explanations. Also, our ensemble produces meaningful visual counterfactuals which are complementary to existing saliency-based techniques. Code is available under https://github.com/valentyn1boreiko/Fundus_VCEs.},
   author = {Valentyn Boreiko and Indu Ilanchezian and Murat Seçkin Ayhan and Sarah Müller and Lisa M. Koch and Hanna Faber and Philipp Berens and Matthias Hein},
   doi = {10.1007/978-3-031-16434-7},
   isbn = {9783031164330},
   issn = {16113349},
   journal = {Proc. International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)},
   keywords = {Adversarial robustness,Counterfactual explanations,Diabetic retinopathy,Interpretability,Trustworthy AI},
   pages = {539-549},
   title = {Visual Explanations for the Detection of Diabetic Retinopathy from Retinal Fundus Images},
   volume = {13432 LNCS},
   year = {2022},
}



@inproceedings{wundram2024pipeline,
   abstract = {The accurate segmentation of the optic cup and disc in fundus images is essential for diagnostic processes such as glaucoma detection. The inherent ambiguity in locating these structures often poses a significant challenge, leading to potential misdiagnosis. To model such ambiguities, numerous probabilistic segmentation models have been proposed. In this paper, we investigate the integration of these probabilistic segmentation models into a mul-tistage pipeline closely resembling clinical practice. Our findings indicate that leveraging the uncertainties provided by these models substantially enhances the quality of glaucoma diagnosis compared to relying on a single segmentation only.},
   author = {Anna M Wundram and Paul Fischer and Stephan Wunderlich and Hanna Faber and Lisa M Koch and Philipp Berens and Christian F Baumgartner},
   journal = {Proc. Medical Imaging with Deep Learning (MIDL)},
   title = {Leveraging Probabilistic Segmentation Models for Improved Glaucoma Diagnosis: A Clinical Pipeline Approach},
   year = {2024},
}
@inproceedings{koehler2024efficiently,
   abstract = {Segmentation models which are deployed into clinical practice need to meet a quality standard for each image. Even when models perform well on average, they may fail at segmenting individual images with a sufficiently high quality. We propose a combined quality control and error correction framework to reach the desired segmentation quality in each image. Our framework recommends the necessary number of local patches for manual review and estimates the impact of the intervention on the Dice Score of the corrected seg-mentation. This allows to trade off segmentation quality against time invested into manual review. We select the patches based on uncertainty maps obtained from an ensemble of segmentation models. We evaluated our method on retinal vessel segmentation on fundus images, where the Dice Score increased substantially after reviewing only a few patches. Our method accurately estimated the review's impact on the Dice Score and we found that our framework controls the quality standard efficiently, i.e. reviewing as little as necessary.},
   author = {Patrick Köhler and Jeremiah Fadugba and Philipp Berens and Lisa M Koch},
   journal = {Proc. Medical Imaging with Deep Learning (MIDL)},
   keywords = {Fundus,Quality Control,Retinal Blood Vessels,Segmentation},
   title = {Efficiently correcting patch-based segmentation errors to control image-level performance in retinal images},
   year = {2024},
}
@inproceedings{koch2022hidden,
   abstract = {The safe application of machine learning systems in healthcare relies on valid performance claims. Such claims are typically established in a clinical validation setting designed to be as close as possible to the intended use, but inadvertent domain or population shifts remain a fundamental problem. In particular, subgroups may be differently represented in the data distribution in the validation compared to the application setting. For example, algorithms trained on population cohort data spanning all age groups may be predominantly applied in elderly people. While these data are not "out-of distribution", changes in the prevalence of different subgroups may have considerable impact on algorithm performance or will at least render original performance claims invalid. Both are serious problems for safely deploying machine learning systems. In this paper, we demonstrate the fundamental limitations of individual example out-of-distribution detection for such scenarios, and show that subgroup shifts can be detected on a population-level instead. We formulate population-level shift detection in the framework of statistical hypothesis testing and show that recent state-of-the-art statistical tests can be effectively applied to subgroup shift detection in a synthetic scenario as well as real histopathology images.},
   author = {Lisa M Koch and Christian M Schürch and Arthur Gretton and Philipp Berens},
   issn = {2640-3498},
   journal = {Proceedings of Machine Learning Research},
   keywords = {domain shift detection,hypothesis testing,kernel methods,safety,subgroups},
   month = {12},
   pages = {726-740},
   publisher = {PMLR},
   title = {Hidden in Plain Sight: Subgroup Shifts Escape OOD Detection},
   volume = {172},
   year = {2022},
}
@article{koch2024postmarket,
   author = {Lisa M Koch and Christian F Baumgartner and Philipp Berens},
   doi = {https://doi.org/10.1038/s41746-024-01085-w},
   journal = {npj Digital Medicine},
   title = {Distribution Shift Detection for the Postmarket Surveillance of Medical AI Algorithms: A Retrospective Simulation Study},
   year = {2024},
}
